{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import mido\n",
    "\n",
    "NOTE_MAX = 119.0\n",
    "\n",
    "def get_message_value(msg):\n",
    "    if msg.type == \"note_off\" or msg.velocity == 0:\n",
    "        return -msg.note / NOTE_MAX\n",
    "    else:\n",
    "        return msg.note / NOTE_MAX\n",
    "\n",
    "def track_to_notes(track):\n",
    "    return [get_message_value(m) for m in track if m.type.startswith(\"note_\")]\n",
    "\n",
    "def get_author_tracks(author):\n",
    "    tracks = []\n",
    "    for dr in glob.glob(\"dataset/\" + author + \"/**\"):\n",
    "        mid = mido.MidiFile(dr)\n",
    "        tracks.extend(mid.tracks)\n",
    "    return tracks\n",
    "\n",
    "def get_author_note_list(author, min_note_count=600):\n",
    "    tracks = get_author_tracks(author)\n",
    "    note_list = [track_to_notes(t) for t in tracks]\n",
    "    return [n for n in note_list if len(n) >= min_note_count]\n",
    "\n",
    "def get_author_min_note_list(note_lists):\n",
    "    lens = [len(t) for t in note_lists]\n",
    "    return min(lens)\n",
    "\n",
    "def divide_note_list_into_inputs(note_list, group_size=600):\n",
    "    chunks = []\n",
    "    chunk_size = len(note_list) // 600\n",
    "    for i in range(0, chunk_size):\n",
    "        chunks.append(note_list[group_size * i: group_size * (i + 1)])\n",
    "    return chunks\n",
    "\n",
    "def get_tracks_chunks(tracks):\n",
    "    chunks = []\n",
    "    for t in tracks:\n",
    "        chunks.extend(divide_note_list_into_inputs(t))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "albeniz_tracks = get_author_note_list(\"Alb√©niz Isaac\")\n",
    "agnew_tracks = get_author_note_list(\"Agnew Roy\")\n",
    "behr_tracks = get_author_note_list(\"Behr Franz\")\n",
    "liszt_tracks = get_author_note_list(\"Liszt Franz\")\n",
    "zierau_tracks = get_author_note_list(\"Zierau Fritz\")\n",
    "frontini_tracks = get_author_note_list(\"Frontini Francesco Paolo\")\n",
    "\n",
    "albeniz_chunks = get_tracks_chunks(albeniz_tracks)\n",
    "agnew_chunks = get_tracks_chunks(agnew_tracks)\n",
    "behr_chunks = get_tracks_chunks(behr_tracks)\n",
    "liszt_chunks = get_tracks_chunks(liszt_tracks)\n",
    "zierau_chunks = get_tracks_chunks(zierau_tracks)\n",
    "frontini_chunks = get_tracks_chunks(frontini_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, ZeroPadding1D, Reshape, GlobalAveragePooling1D, Dropout\n",
    "\n",
    "def halve_array(arr):\n",
    "    div = len(arr) // 2\n",
    "    return arr[:div], arr[div:]\n",
    "\n",
    "def create_labels(label, leng):\n",
    "    labels = []\n",
    "    for _ in range(leng):\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "def generate_data_from_chunks(chunks, label):\n",
    "    train, test = halve_array(chunks)\n",
    "    train_labels = create_labels(label, len(train))\n",
    "    test_labels = create_labels(label, len(test))\n",
    "\n",
    "    return {\n",
    "        \"train_data\": train,\n",
    "        \"test_data\": test,\n",
    "        \"train_labels\": train_labels,\n",
    "        \"test_labels\": test_labels\n",
    "    }\n",
    "\n",
    "def train_data_from_datasets(datasets):\n",
    "    data = []\n",
    "    for d in datasets:\n",
    "        data += d[\"train_data\"] + d[\"test_data\"]\n",
    "    return np.array(data)\n",
    "\n",
    "def train_labels_from_datasets(datasets):\n",
    "    data = []\n",
    "    for d in datasets:\n",
    "        data += d[\"train_labels\"] + d[\"test_labels\"]\n",
    "    return np.array(data)\n",
    "\n",
    "input_dim = len(albeniz_chunks[0])\n",
    "\n",
    "def test_data_from_datasets(datasets):\n",
    "    data = []\n",
    "    for d in datasets:\n",
    "        data += d[\"test_data\"]\n",
    "    return np.array(data)\n",
    "\n",
    "def test_labels_from_datasets(datasets):\n",
    "    data = []\n",
    "    for d in datasets:\n",
    "        data += d[\"test_labels\"]\n",
    "    return np.array(data)\n",
    "\n",
    "input_dim = len(albeniz_chunks[0])\n",
    "\n",
    "albeniz_dataset = generate_data_from_chunks(albeniz_chunks, [1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "behr_dataset = generate_data_from_chunks(behr_chunks, [0.0, 1.0, 0.0, 0.0, 0.0, 0.0])\n",
    "frontini_dataset = generate_data_from_chunks(frontini_chunks, [0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n",
    "liszt_dataset = generate_data_from_chunks(liszt_chunks, [0.0, 0.0, 0.0, 1.0, 0.0, 0.0])\n",
    "zierau_dataset = generate_data_from_chunks(zierau_chunks, [0.0, 0.0, 0.0, 0.0, 1.0, 0.0])\n",
    "agnew_dataset = generate_data_from_chunks(agnew_chunks, [0.0, 0.0, 0.0, 0.0, 0.0, 1.0])\n",
    "datasets = [albeniz_dataset, behr_dataset, frontini_dataset, liszt_dataset, zierau_dataset, agnew_dataset]\n",
    "\n",
    "train_data = train_data_from_datasets(datasets)\n",
    "train_labels = train_labels_from_datasets(datasets)\n",
    "\n",
    "# test_data = test_data_from_datasets(datasets)\n",
    "# test_labels = test_labels_from_datasets(datasets)\n",
    "\n",
    "final_train, final_labels = shuffle(train_data, train_labels)\n",
    "\n",
    "model = Sequential([\n",
    "    Reshape((30, 20), input_shape=(input_dim, )),\n",
    "    Conv1D(30, 10, activation=\"relu\", input_shape=((30, 20))),\n",
    "    MaxPooling1D(3),\n",
    "    Conv1D(10, 2, activation='relu'),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.003)\n",
    "model.compile(\n",
    "  optimizer=opt,\n",
    "  loss=\"mse\",\n",
    "  metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  final_train,\n",
    "  final_labels,\n",
    "  validation_split=0.2,\n",
    "  epochs=200,\n",
    "  shuffle=True\n",
    ")\n",
    "\n",
    "test_01 = np.array([\n",
    "    albeniz_dataset[\"test_data\"][0],\n",
    "])\n",
    "\n",
    "print(model.predict(test_01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
